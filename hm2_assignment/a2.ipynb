{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Instructions: Implement both PG and an evolutionary algorithm to solve the Open AI Gym Lunar Lander problem, and then apply it to my area of choice, which is chess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to do some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # üßÆ\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = \"mps\" # üß†\n",
    "else:\n",
    "    device = \"cpu\" # ü•∫\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to write the code for our Policy Gradient function with a baseline (taken from REINFORCE). I'm going to use PyTorch as my neural network library (I want to try JAX, but this is the more practical choice for me at the moment. Exploration-Exploitation tradeoff ü§∑‚Äç‚ôÇÔ∏è).\n",
    "\n",
    "I'm going to start with a basic feed forward net for both the network that chooses the policy and the network that learns states' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the policy network for choosing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PolicyChoice(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyChoice, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(8, 8)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(8, 4)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_weighted1 = self.layer1(x)\n",
    "        h1 = self.relu1(x_weighted1)\n",
    "        x_weighted2 = self.layer2(h1)\n",
    "        h2 = self.relu2(x_weighted2)\n",
    "        logits = self.layer3(h2)\n",
    "        return logits\n",
    "\n",
    "policy_model = PolicyChoice().to(device)\n",
    "policy_adam = torch.optim.Adam(policy_model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our loss function for the policy network, we want to adjust just the parameters with the primary aim of affecting the probability of taking the action that we took on that time step. If the return of the resulting state is better than expected, we want to increase it proportionally. If it is less than expected, we want to decrease it proportionally. Thus, we multiply the gradient of the parameter weights w.r.t. the taken action's probability by the difference of the return for that state-action pair.\n",
    "\n",
    "Importantly, there is an extra factor however that we must consider; when we decide that we want to take the gradient of the parameters w.r.t. a specific action's return, the policy expectancy must be multiplied by the specific action's likelihood to determine the value it contributes to the policy. Thus, we end up with the gradient of the action's probability conditioned on the state and parameters. \n",
    "\n",
    "Thus, the general concept of loss to backpropogate in the REINFORCE algorithm is:\n",
    "\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\frac{\\nabla\\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)}$\n",
    "\n",
    "This can be expressed as:\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\nabla \\ln{\\pi(A_t|S_t, \\theta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below just worries about the loss and not the gradient, as PyTorch provides autograd differentiation behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(logits, action_chosen, state_util_difference):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    # NOTE: I think RL literature typically describes problems as gradient ascent\n",
    "    # however I am defining it here as a loss function so we will multiply\n",
    "    # the state's return by -1 and instead aim to minimize this function\n",
    "    # i.e. we will do gradient descent\n",
    "    # TODO: if working and I have time, refactor to gradient ascent\n",
    "    action_scaled = torch.mul(ce_loss(logits, action_chosen), torch.mul(torch.unsqueeze(state_util_difference, dim=1), -1))\n",
    "    return action_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the network for approximating state utililities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateUtility(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StateUtility, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(8, 4)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(4, 2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(2, 1)\n",
    "        # self.relu3 = torch.nn.ReLU()\n",
    "        # self.layer4 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_weighted = self.layer1(x)\n",
    "        h1 = self.relu1(x_weighted)\n",
    "        h1_weighted = self.layer2(h1)\n",
    "        h2 = self.relu2(h1_weighted)\n",
    "        # h2_weighted = self.layer3(h2)\n",
    "        # h3 = self.relu3(h2_weighted)\n",
    "        state_utility = self.layer3(h2)\n",
    "        return state_utility\n",
    "\n",
    "state_util_model = StateUtility().to(device)\n",
    "state_util_adam = torch.optim.Adam(params=state_util_model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the state utilities network, we just use L1 loss with the gradients of W with respect to state utility.\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}(S_t, W)) \\nabla \\hat{\\upsilon}(S_t, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, the code below just worries about the loss and not the gradient, as PyTorch provides autograd differntiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_util_loss(calculated_state_value, episode_state_value):\n",
    "    # the overall state value is the input, and the individual state value is our target\n",
    "    l1_loss = nn.L1Loss(reduction='none')\n",
    "    return l1_loss(calculated_state_value, episode_state_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having limitied compute (and longer runtimes) led to me to reflect on gamma tuning. I initially noticed that having a higher discount factor (smaller gamma) improved values a lot for this Lunar Lander task. My first thought was that it's a reflection of the fact that this task has very well-defined rewards that are frequent and that reflect short-term actions.\n",
    "\n",
    "That's definitely true, but as I reflected on it, it also occurred to me that using a higher discount factor is, in general, a tradeoff. With a higher discount factor you are hamstringing your ability to learn long-term dependencies, but you can learn action's values much faster. However with a very low discount factor you can actually still theoretically learn actions fine grained values and not conflate them, but it just takes a lot more training examples as the Monte Carlo nature of sampling will eventually lead to distinction (my models with higher gammas seemed to still be learning when they terminated). And with this lower factor you can also learn long-term dependencies fairly easy.\n",
    "\n",
    "I interpreted this as meaning that I should leave gamma a little higher than it's best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma = .4\n",
    "\n",
    "gamma = .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the Lunar Lander environment now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: use a custom dataloader class and see if speed up\n",
    "\n",
    "# env = gym.make(\n",
    "#     \"LunarLander-v2\",\n",
    "#     #render_mode=\"human\",\n",
    "#     enable_wind=False,\n",
    "# )\n",
    "\n",
    "# env_2 = gym.make(\n",
    "#     \"LunarLander-v2\",\n",
    "#     render_mode=\"human\",\n",
    "#     enable_wind=False,\n",
    "    \n",
    "# )\n",
    "\n",
    "# num_of_actions = 4\n",
    "\n",
    "# action_space_seed = np.random.seed(13)\n",
    "\n",
    "# observation, info = env.reset(seed=13)\n",
    "\n",
    "# episodes_total_rewards_sums = []\n",
    "# # for debug of state-value funtion\n",
    "# episode_total_state_err = []\n",
    "\n",
    "# observations = []\n",
    "# # NOTE: rewards[0] corresponds to the result of calc_reward(state_of(observations[0]), action_indices[0])\n",
    "# # thus len(rewards) == len(action_indices) == len(observations) - 1\n",
    "# # i.e. no reward for the first timestep, no action_index for the last timestep\n",
    "# rewards = []\n",
    "# action_indices = []\n",
    "# action_logits_per_ep = []\n",
    "# # for debug of state-value funtion\n",
    "# state_preds = []\n",
    "# state_err = []\n",
    "\n",
    "# policy_adam.zero_grad()\n",
    "# state_util_adam.zero_grad()\n",
    "\n",
    "# #warmup, policy frozen\n",
    "# for timestep in range(1300000):\n",
    "\n",
    "#     if timestep==0 or timestep==900000:\n",
    "#         print('debug entry')\n",
    "    \n",
    "#     # use policy gradient to get action probabilities; sample stochastically\n",
    "#     action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "#     with torch.no_grad():\n",
    "#         action_logits_per_ep.append(action_logits.detach().clone())\n",
    "#         action_probs = torch.nn.functional.softmax(action_logits, dim=0)\n",
    "#         action_sampling = torch.multinomial(action_probs, 1)\n",
    "#         action = action_sampling.item()\n",
    "#         action_indices.append(action)\n",
    "    \n",
    "#     observations.append(observation)\n",
    "#     # get info from environment\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "#     rewards.append(reward)\n",
    "    \n",
    "#     # end of episode\n",
    "#     if terminated or truncated:\n",
    "#         observations.append(observation)\n",
    "#         ep_length = len(observations[:-1]) # Do not take the terminal state as we have no action in the terminal state\n",
    "#         ep_total_rewards_sum = np.sum(np.array(rewards))\n",
    "#         episodes_total_rewards_sums.append(ep_total_rewards_sum)\n",
    "#         returns = np.zeros(len(observations) - 1)\n",
    "#         for timestep in reversed(range(ep_length)):\n",
    "\n",
    "#             # calculate state's actual return by looking at reward + future rewards\n",
    "#             terminal = timestep == len(rewards) - 1\n",
    "#             returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             actual_state_util = torch.zeros((len(returns), 1), device=device)\n",
    "#             for i, actual_util in enumerate(returns):\n",
    "#                 actual_state_util[i] = torch.tensor(returns[i], device=device)\n",
    "#             # calculate baseline expected state value\n",
    "#             input_state_util = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "#             for i, input_samples in enumerate(observations[:-1]):\n",
    "#                 input_state_util[i] = torch.tensor(observations[i], device=device)\n",
    "#         pred_state_util = state_util_model(input_state_util)\n",
    "#         loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "#         with torch.no_grad():\n",
    "#             state_pred_err = np.abs(loss_state_utility.detach().clone().mean().item())\n",
    "#             state_preds.append(pred_state_util.detach().clone())\n",
    "#             state_err.append(state_pred_err)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             state_util_differences = []\n",
    "#             for timestep in range(ep_length):\n",
    "#                 # make updates to policy (specific action) based on return\n",
    "#                 # get the state's return minus the baseline (predicted state return)\n",
    "#                 state_util_differences.append(actual_state_util.detach().clone()[timestep] - pred_state_util.detach().clone()[timestep])\n",
    "        \n",
    "#             # TODO: change to store rather than recomputation, but without autograd complaining about inplace operations\n",
    "#             # e.g. putting the tensor in a list\n",
    "#         # with torch.no_grad():\n",
    "#         #     input_policy = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "#         #     for i, input_samples in enumerate(observations[:-1]):\n",
    "#         #         input_policy[i] = torch.tensor(observations[i], device=device)\n",
    "#         #     actions_chosen_tensor = torch.zeros((len(action_indices), num_of_actions), device=device)\n",
    "#         #     for i, action_index in enumerate(action_indices):\n",
    "#         #         actions_chosen_tensor[i][action_index] = 1\n",
    "#         #     state_util_diffs_tensor = torch.tensor(state_util_differences, device=device)\n",
    "#         # recomputed_policy = policy_model(input_state_util)\n",
    "#         # loss_policy = policy_loss(recomputed_policy, actions_chosen_tensor, state_util_diffs_tensor)\n",
    "\n",
    "#         episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "#         # accumulate, avg, and add gradients to parameters for state value network\n",
    "#         loss_state_utility.mean().backward()\n",
    "#         state_util_adam.step()\n",
    "#         state_util_adam.zero_grad()\n",
    "    \n",
    "#         # accumulate, avg, and add gradients to parameters for policy network\n",
    "#         # loss_policy.mean().backward()\n",
    "#         # policy_adam.step()\n",
    "#         # policy_adam.zero_grad()\n",
    "\n",
    "#         observation, info = env.reset()\n",
    "#         observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "#         state_err, state_preds = [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(f'The avg. state val prediction error on the first quarter of episodes was: {np.sum(episode_total_state_err[:len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "# print(f'The avg. state val prediction error on the second quarter of episodes was: {np.sum(episode_total_state_err[len(episode_total_state_err)//4:2 * len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "# print(f'The avg. state val prediction error on the third quarter of episodes was: {np.sum(episode_total_state_err[2 * len(episode_total_state_err)//4:3 *len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "# print(f'The avg. state val prediction error on the fourth quarter of episodes was: {np.sum(episode_total_state_err[3 *len(episode_total_state_err)//4:len(episode_total_state_err)]) / (len(episode_total_state_err)/4)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# episodes_total_rewards_sums = []\n",
    "# ep_total_rewards = []\n",
    "# # for debug of state-value funtion\n",
    "# episode_total_state_err = []\n",
    "# action_logits_episodes = []\n",
    "# observations, rewards, returns, action_indices, action_logits_per_ep = [], [], [], [], []\n",
    "# state_err, state_preds = [], []\n",
    "# observation, info = env.reset()\n",
    "\n",
    "# # state_util_adam = torch.optim.Adam(params=state_util_model.parameters(), lr=1e-3, weight_decay=)\n",
    "\n",
    "# # everything unfrozen\n",
    "# for timestep in range(1000000):\n",
    "\n",
    "#     if timestep==0 or timestep==900000:\n",
    "#         print('debug entry')\n",
    "    \n",
    "#     # use policy gradient to get action probabilities; sample stochastically\n",
    "#     action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "#     with torch.no_grad():\n",
    "#         action_logits_per_ep.append(action_logits.detach().clone())\n",
    "#         action_probs = torch.nn.functional.softmax(action_logits, dim=0)\n",
    "#         action_sampling = torch.multinomial(action_probs, 1)\n",
    "#         action = action_sampling.item()\n",
    "#         action_indices.append(action)\n",
    "    \n",
    "#     observations.append(observation)\n",
    "#     # get info from environment\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "#     rewards.append(reward)\n",
    "    \n",
    "#     # end of episode\n",
    "#     if terminated or truncated:\n",
    "#         observations.append(observation)\n",
    "#         ep_length = len(observations[:-1]) # Do not take the terminal state as we have no action in the terminal state\n",
    "#         ep_total_rewards_sum = np.sum(np.array(rewards))\n",
    "#         ep_total_rewards.append(rewards)\n",
    "#         episodes_total_rewards_sums.append(ep_total_rewards_sum)\n",
    "#         returns = np.zeros(len(observations) - 1)\n",
    "#         for timestep in reversed(range(ep_length)):\n",
    "\n",
    "#             # calculate state's actual return by looking at reward + future rewards\n",
    "#             terminal = timestep == len(rewards) - 1\n",
    "#             returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             actual_state_util = torch.zeros((len(returns), 1), device=device)\n",
    "#             for i, actual_util in enumerate(returns):\n",
    "#                 actual_state_util[i] = torch.tensor(returns[i], device=device)\n",
    "#             # calculate baseline expected state value\n",
    "#             input_state_util = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "#             for i, input_samples in enumerate(observations[:-1]):\n",
    "#                 input_state_util[i] = torch.tensor(observations[i], device=device)\n",
    "#         pred_state_util = state_util_model(input_state_util)\n",
    "#         loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "        \n",
    "#         # some extra info helpful for debug\n",
    "#         with torch.no_grad():\n",
    "#             state_pred_err = np.abs(loss_state_utility.detach().clone().mean().item())\n",
    "#             state_preds.append(pred_state_util.detach().clone())\n",
    "#             state_err.append(state_pred_err)\n",
    "#             state_util_differences = []\n",
    "#             for timestep in range(ep_length):\n",
    "#                 # make updates to policy (specific action) based on return\n",
    "#                 # get the state's return minus the baseline (predicted state return)\n",
    "#                 state_util_differences.append(actual_state_util.detach().clone()[timestep] - pred_state_util.detach().clone()[timestep])\n",
    "        \n",
    "#             # TODO: change to store rather than recomputation, but without autograd complaining about inplace operations\n",
    "#             # e.g. putting the tensor in a list\n",
    "#         with torch.no_grad():\n",
    "#             input_policy = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "#             for i, input_samples in enumerate(observations[:-1]):\n",
    "#                 input_policy[i] = torch.tensor(observations[i], device=device)\n",
    "#             actions_chosen_tensor = torch.zeros((len(action_indices), num_of_actions), device=device)\n",
    "#             for i, action_index in enumerate(action_indices):\n",
    "#                 actions_chosen_tensor[i][action_index] = 1\n",
    "#             state_util_diffs_tensor = torch.tensor(state_util_differences, device=device)\n",
    "#         recomputed_policy = policy_model(input_state_util)\n",
    "#         loss_policy = policy_loss(recomputed_policy, actions_chosen_tensor, state_util_diffs_tensor)\n",
    "\n",
    "#         episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "#         # accumulate, avg, and add gradients to parameters for state value network\n",
    "#         loss_state_utility.mean().backward()\n",
    "#         state_util_adam.step()\n",
    "#         state_util_adam.zero_grad()\n",
    "    \n",
    "#         # accumulate, avg, and add gradients to parameters for policy network\n",
    "#         loss_policy.mean().backward()\n",
    "#         policy_adam.step()\n",
    "#         policy_adam.zero_grad()\n",
    "\n",
    "#         observation, info = env.reset()\n",
    "#         action_logits_episodes.append(action_logits_per_ep)\n",
    "#         observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "#         state_err, state_preds = [], []\n",
    "\n",
    "# # TODO: move these to a self-contained function\n",
    "\n",
    "# print(f'The avg. state val prediction error on the first quarter of episodes was: {np.sum(episode_total_state_err[:len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "# print(f'The avg. state val prediction error on the second quarter of episodes was: {np.sum(episode_total_state_err[len(episode_total_state_err)//4:2 * len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "# print(f'The avg. state val prediction error on the third quarter of episodes was: {np.sum(episode_total_state_err[2 * len(episode_total_state_err)//4:3 *len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "# print(f'The avg. state val prediction error on the fourth quarter of episodes was: {np.sum(episode_total_state_err[3 *len(episode_total_state_err)//4:len(episode_total_state_err)]) / (len(episode_total_state_err)/4)}')\n",
    "\n",
    "# print(f'The avg. episode reward on the first quarter of episodes was: {np.sum(episodes_total_rewards_sums[:len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)//4)}')\n",
    "# print(f'The avg. episode reward on the second quarter of episodes was: {np.sum(episodes_total_rewards_sums[len(episodes_total_rewards_sums)//4:2 * len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "# print(f'The avg. episode reward on the third quarter of episodes was: {np.sum(episodes_total_rewards_sums[2 * len(episodes_total_rewards_sums)//4:3 *len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "# print(f'The avg. episode reward on the fourth quarter of episodes was: {np.sum(episodes_total_rewards_sums[3 *len(episodes_total_rewards_sums)//4:len(episodes_total_rewards_sums)]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "\n",
    "\n",
    "# observation, info = env_2.reset(seed=45)\n",
    "\n",
    "# observations, rewards, returns, action_indices, action_logits_per_ep = [], [], [], [], []\n",
    "\n",
    "# episodes_total_rewards = []\n",
    "\n",
    "# # just to see what the model has learnt visually\n",
    "# for timestep in range(1000000):\n",
    "\n",
    "#     if timestep==0 or timestep==900000:\n",
    "#         print('debug entry')\n",
    "    \n",
    "#     # use policy gradient to get action probabilities; sample stochastically\n",
    "#     action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "#     with torch.no_grad():\n",
    "#         action_logits_per_ep.append(action_logits.detach().clone())\n",
    "#         action_probs = torch.nn.functional.softmax(action_logits, dim=0)\n",
    "#         action_sampling = torch.multinomial(action_probs, 1)\n",
    "#         action = action_sampling.item()\n",
    "#         action_indices.append(action)\n",
    "    \n",
    "#     observations.append(observation)\n",
    "#     # get info from environment\n",
    "#     observation, reward, terminated, truncated, info = env_2.step(action)\n",
    "#     rewards.append(reward)\n",
    "    \n",
    "#     # end of episode\n",
    "#     if terminated or truncated:\n",
    "#         observations.append(observation)\n",
    "#         ep_length = len(observations[:-1]) # Do not take the terminal state as we have no action in the terminal state\n",
    "#         ep_total_reward = np.sum(np.array(rewards))\n",
    "#         episodes_total_rewards.append(ep_total_reward)\n",
    "#         returns = np.zeros(len(observations) - 1)\n",
    "#         for timestep in reversed(range(ep_length)):\n",
    "\n",
    "#             # calculate state's actual return by looking at reward + future rewards\n",
    "#             terminal = timestep == len(rewards) - 1\n",
    "#             returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             actual_state_util = torch.zeros((len(returns), 1), device=device)\n",
    "#             for i, actual_util in enumerate(returns):\n",
    "#                 actual_state_util[i] = torch.tensor(returns[i], device=device)\n",
    "#             # calculate baseline expected state value\n",
    "#             input_state_util = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "#             for i, input_samples in enumerate(observations[:-1]):\n",
    "#                 input_state_util[i] = torch.tensor(observations[i], device=device)\n",
    "#         pred_state_util = state_util_model(input_state_util)\n",
    "#         loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "        \n",
    "#         # some extra info helpful for debug\n",
    "#         with torch.no_grad():\n",
    "#             state_pred_err = np.abs(loss_state_utility.detach().clone().mean().item())\n",
    "#             state_preds.append(pred_state_util.detach().clone())\n",
    "#             state_err.append(state_pred_err)\n",
    "#             state_util_differences = []\n",
    "#             for timestep in range(ep_length):\n",
    "#                 # make updates to policy (specific action) based on return\n",
    "#                 # get the state's return minus the baseline (predicted state return)\n",
    "#                 state_util_differences.append(actual_state_util.detach().clone()[timestep] - pred_state_util.detach().clone()[timestep])\n",
    "        \n",
    "#             # TODO: change to store rather than recomputation, but without autograd complaining about inplace operations\n",
    "#             # e.g. putting the tensor in a list\n",
    "#         with torch.no_grad():\n",
    "#             input_policy = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "#             for i, input_samples in enumerate(observations[:-1]):\n",
    "#                 input_policy[i] = torch.tensor(observations[i], device=device)\n",
    "#             actions_chosen_tensor = torch.zeros((len(action_indices), num_of_actions), device=device)\n",
    "#             for i, action_index in enumerate(action_indices):\n",
    "#                 actions_chosen_tensor[i][action_index] = 1\n",
    "#             state_util_diffs_tensor = torch.tensor(state_util_differences, device=device)\n",
    "#         recomputed_policy = policy_model(input_state_util)\n",
    "#         loss_policy = policy_loss(recomputed_policy, actions_chosen_tensor, state_util_diffs_tensor)\n",
    "\n",
    "#         episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "#         # accumulate, avg, and add gradients to parameters for state value network\n",
    "#         loss_state_utility.sum().backward()\n",
    "#         state_util_adam.step()\n",
    "#         state_util_adam.zero_grad()\n",
    "    \n",
    "#         # accumulate, avg, and add gradients to parameters for policy network\n",
    "#         loss_policy.sum().backward()\n",
    "#         policy_adam.step()\n",
    "#         policy_adam.zero_grad()\n",
    "\n",
    "#         observation, info = env_2.reset()\n",
    "#         observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "#         state_err, state_preds = [], []\n",
    "\n",
    "\n",
    "# env_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-batched below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use a custom dataloader class and see if speed up\n",
    "\n",
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    #render_mode=\"human\",\n",
    "    enable_wind=False,\n",
    ")\n",
    "\n",
    "env_2 = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    render_mode=\"human\",\n",
    "    enable_wind=False,\n",
    "    \n",
    ")\n",
    "\n",
    "num_of_actions = 4\n",
    "\n",
    "action_space_seed = np.random.seed(13)\n",
    "\n",
    "observation, info = env.reset(seed=13)\n",
    "\n",
    "episodes_total_rewards_sums = []\n",
    "# for debug of state-value funtion\n",
    "episode_total_state_err = []\n",
    "\n",
    "observations = []\n",
    "# NOTE: rewards[0] corresponds to the result of calc_reward(state_of(observations[0]), action_indices[0])\n",
    "# thus len(rewards) == len(action_indices) == len(observations) - 1\n",
    "# i.e. no reward for the first timestep, no action_index for the last timestep\n",
    "rewards = []\n",
    "action_indices = []\n",
    "action_logits_per_ep = []\n",
    "# for debug of state-value funtion\n",
    "state_preds = []\n",
    "state_err = []\n",
    "\n",
    "policy_adam.zero_grad()\n",
    "state_util_adam.zero_grad()\n",
    "\n",
    "#warmup, policy frozen\n",
    "for timestep in range(1300000):\n",
    "\n",
    "    if timestep==0 or timestep==900000:\n",
    "        print('debug entry')\n",
    "    \n",
    "    # use policy gradient to get action probabilities; sample stochastically\n",
    "    action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "    with torch.no_grad():\n",
    "        action_logits_per_ep.append(action_logits.detach().clone())\n",
    "        action_probs = torch.nn.functional.softmax(action_logits, dim=0)\n",
    "        action_sampling = torch.multinomial(action_probs, 1)\n",
    "        action = action_sampling.item()\n",
    "        action_indices.append(action)\n",
    "    \n",
    "    observations.append(observation)\n",
    "    # get info from environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # end of episode\n",
    "    if terminated or truncated:\n",
    "        observations.append(observation)\n",
    "        ep_length = len(observations[:-1]) # Do not take the terminal state as we have no action in the terminal state\n",
    "        ep_total_rewards_sum = np.sum(np.array(rewards))\n",
    "        episodes_total_rewards_sums.append(ep_total_rewards_sum)\n",
    "        returns = np.zeros(len(observations) - 1)\n",
    "        for timestep in reversed(range(ep_length)):\n",
    "\n",
    "            # calculate state's actual return by looking at reward + future rewards\n",
    "            terminal = timestep == len(rewards) - 1\n",
    "            returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actual_state_util = torch.zeros((len(returns), 1), device=device)\n",
    "            for i, actual_util in enumerate(returns):\n",
    "                actual_state_util[i] = torch.tensor(returns[i], device=device)\n",
    "            # calculate baseline expected state value\n",
    "            input_state_util = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            for i, input_samples in enumerate(observations[:-1]):\n",
    "                input_state_util[i] = torch.tensor(observations[i], device=device)\n",
    "        pred_state_util = state_util_model(input_state_util)\n",
    "        loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "        with torch.no_grad():\n",
    "            state_pred_err = np.abs(loss_state_utility.detach().clone().mean().item())\n",
    "            state_preds.append(pred_state_util.detach().clone())\n",
    "            state_err.append(state_pred_err)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_util_differences = []\n",
    "            for timestep in range(ep_length):\n",
    "                # make updates to policy (specific action) based on return\n",
    "                # get the state's return minus the baseline (predicted state return)\n",
    "                state_util_differences.append(actual_state_util.detach().clone()[timestep] - pred_state_util.detach().clone()[timestep])\n",
    "        \n",
    "            # TODO: change to store rather than recomputation, but without autograd complaining about inplace operations\n",
    "            # e.g. putting the tensor in a list\n",
    "        # with torch.no_grad():\n",
    "        #     input_policy = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "        #     for i, input_samples in enumerate(observations[:-1]):\n",
    "        #         input_policy[i] = torch.tensor(observations[i], device=device)\n",
    "        #     actions_chosen_tensor = torch.zeros((len(action_indices), num_of_actions), device=device)\n",
    "        #     for i, action_index in enumerate(action_indices):\n",
    "        #         actions_chosen_tensor[i][action_index] = 1\n",
    "        #     state_util_diffs_tensor = torch.tensor(state_util_differences, device=device)\n",
    "        # recomputed_policy = policy_model(input_state_util)\n",
    "        # loss_policy = policy_loss(recomputed_policy, actions_chosen_tensor, state_util_diffs_tensor)\n",
    "\n",
    "        episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "        # accumulate, avg, and add gradients to parameters for state value network\n",
    "        loss_state_utility.mean().backward()\n",
    "        state_util_adam.step()\n",
    "        state_util_adam.zero_grad()\n",
    "    \n",
    "        # accumulate, avg, and add gradients to parameters for policy network\n",
    "        # loss_policy.mean().backward()\n",
    "        # policy_adam.step()\n",
    "        # policy_adam.zero_grad()\n",
    "\n",
    "        observation, info = env.reset()\n",
    "        observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "        state_err, state_preds = [], []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'The avg. state val prediction error on the first quarter of episodes was: {np.sum(episode_total_state_err[:len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the second quarter of episodes was: {np.sum(episode_total_state_err[len(episode_total_state_err)//4:2 * len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the third quarter of episodes was: {np.sum(episode_total_state_err[2 * len(episode_total_state_err)//4:3 *len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the fourth quarter of episodes was: {np.sum(episode_total_state_err[3 *len(episode_total_state_err)//4:len(episode_total_state_err)]) / (len(episode_total_state_err)/4)}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "episodes_total_rewards_sums = []\n",
    "ep_total_rewards = []\n",
    "# for debug of state-value funtion\n",
    "episode_total_state_err = []\n",
    "action_logits_episodes = []\n",
    "observations, rewards, returns, action_indices, action_logits_per_ep = [], [], [], [], []\n",
    "state_err, state_preds = [], []\n",
    "observation, info = env.reset()\n",
    "\n",
    "# state_util_adam = torch.optim.Adam(params=state_util_model.parameters(), lr=1e-3, weight_decay=)\n",
    "\n",
    "# everything unfrozen\n",
    "for timestep in range(1000000):\n",
    "\n",
    "    if timestep==0 or timestep==900000:\n",
    "        print('debug entry')\n",
    "    \n",
    "    # use policy gradient to get action probabilities; sample stochastically\n",
    "    action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "    with torch.no_grad():\n",
    "        action_logits_per_ep.append(action_logits.detach().clone())\n",
    "        action_probs = torch.nn.functional.softmax(action_logits, dim=0)\n",
    "        action_sampling = torch.multinomial(action_probs, 1)\n",
    "        action = action_sampling.item()\n",
    "        action_indices.append(action)\n",
    "    \n",
    "    observations.append(observation)\n",
    "    # get info from environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # end of episode\n",
    "    if terminated or truncated:\n",
    "        observations.append(observation)\n",
    "        ep_length = len(observations[:-1]) # Do not take the terminal state as we have no action in the terminal state\n",
    "        ep_total_rewards_sum = np.sum(np.array(rewards))\n",
    "        ep_total_rewards.append(rewards)\n",
    "        episodes_total_rewards_sums.append(ep_total_rewards_sum)\n",
    "        returns = np.zeros(len(observations) - 1)\n",
    "        for timestep in reversed(range(ep_length)):\n",
    "\n",
    "            # calculate state's actual return by looking at reward + future rewards\n",
    "            terminal = timestep == len(rewards) - 1\n",
    "            returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     actual_state_util = torch.zeros((len(returns), 1), device=device)\n",
    "        for i, actual_util in enumerate(returns):\n",
    "        #         actual_state_util[i] = torch.tensor(returns[i], device=device)\n",
    "            actual_state_util = torch.tensor(returns[i], device=device)\n",
    "            # calculate baseline expected state value\n",
    "            # input_state_util = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            # for i, input_samples in enumerate(observations[:-1]):\n",
    "            #     input_state_util[i] = torch.tensor(observations[i], device=device)\n",
    "            input_state_util = torch.tensor(observations[i], device=device)\n",
    "            pred_state_util = state_util_model(input_state_util)\n",
    "            loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "        \n",
    "        # some extra info helpful for debug\n",
    "            with torch.no_grad():\n",
    "        #     state_pred_err = np.abs(loss_state_utility.detach().clone().mean().item())\n",
    "        #     state_preds.append(pred_state_util.detach().clone())\n",
    "        #     state_err.append(state_pred_err)\n",
    "                state_util_differences = []\n",
    "        #     for timestep in range(ep_length):\n",
    "        #         # make updates to policy (specific action) based on return\n",
    "        #         # get the state's return minus the baseline (predicted state return)\n",
    "                state_util_differences.append(actual_state_util.detach().clone() - pred_state_util.detach().clone())\n",
    "        \n",
    "            # TODO: change to store rather than recomputation, but without autograd complaining about inplace operations\n",
    "            # e.g. putting the tensor in a list\n",
    "            with torch.no_grad():\n",
    "            # #input_policy = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            #     for i, input_samples in enumerate(observations[:-1]):\n",
    "                input_policy = torch.tensor(observations[i], device=device)\n",
    "                actions_chosen_tensor = torch.zeros(4, device=device)\n",
    "                actions_chosen_tensor[action_indices[timestep]] = 1\n",
    "                state_util_diffs_tensor = torch.tensor(state_util_differences[timestep], device=device)\n",
    "            recomputed_policy = policy_model(input_state_util)\n",
    "            loss_policy = policy_loss(recomputed_policy, actions_chosen_tensor, state_util_diffs_tensor)\n",
    "\n",
    "            loss_policy.mean().backward()\n",
    "            policy_adam.step()\n",
    "            policy_adam.zero_grad()\n",
    "\n",
    "            loss_state_utility.mean().backward()\n",
    "            state_util_adam.step()\n",
    "            state_util_adam.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "        episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "    \n",
    "        # # accumulate, avg, and add gradients to parameters for policy network\n",
    "        # loss_policy.mean().backward()\n",
    "        # policy_adam.step()\n",
    "        # policy_adam.zero_grad()\n",
    "\n",
    "        # accumulate, avg, and add gradients to parameters for state value network\n",
    "        # loss_state_utility.mean().backward()\n",
    "        # state_util_adam.step()\n",
    "        # state_util_adam.zero_grad()\n",
    "\n",
    "        observation, info = env.reset()\n",
    "        action_logits_episodes.append(action_logits_per_ep)\n",
    "        observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "        state_err, state_preds = [], []\n",
    "\n",
    "# TODO: move these to a self-contained function\n",
    "\n",
    "print(f'The avg. state val prediction error on the first quarter of episodes was: {np.sum(episode_total_state_err[:len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the second quarter of episodes was: {np.sum(episode_total_state_err[len(episode_total_state_err)//4:2 * len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the third quarter of episodes was: {np.sum(episode_total_state_err[2 * len(episode_total_state_err)//4:3 *len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the fourth quarter of episodes was: {np.sum(episode_total_state_err[3 *len(episode_total_state_err)//4:len(episode_total_state_err)]) / (len(episode_total_state_err)/4)}')\n",
    "\n",
    "print(f'The avg. episode reward on the first quarter of episodes was: {np.sum(episodes_total_rewards_sums[:len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)//4)}')\n",
    "print(f'The avg. episode reward on the second quarter of episodes was: {np.sum(episodes_total_rewards_sums[len(episodes_total_rewards_sums)//4:2 * len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "print(f'The avg. episode reward on the third quarter of episodes was: {np.sum(episodes_total_rewards_sums[2 * len(episodes_total_rewards_sums)//4:3 *len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "print(f'The avg. episode reward on the fourth quarter of episodes was: {np.sum(episodes_total_rewards_sums[3 *len(episodes_total_rewards_sums)//4:len(episodes_total_rewards_sums)]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "\n",
    "\n",
    "observation, info = env_2.reset(seed=45)\n",
    "\n",
    "observations, rewards, returns, action_indices, action_logits_per_ep = [], [], [], [], []\n",
    "\n",
    "episodes_total_rewards = []\n",
    "\n",
    "# just to see what the model has learnt visually\n",
    "for timestep in range(1000000):\n",
    "\n",
    "    if timestep==0 or timestep==900000:\n",
    "        print('debug entry')\n",
    "    \n",
    "    # use policy gradient to get action probabilities; sample stochastically\n",
    "    action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "    with torch.no_grad():\n",
    "        action_logits_per_ep.append(action_logits.detach().clone())\n",
    "        action_probs = torch.nn.functional.softmax(action_logits, dim=0)\n",
    "        action_sampling = torch.multinomial(action_probs, 1)\n",
    "        action = action_sampling.item()\n",
    "        action_indices.append(action)\n",
    "    \n",
    "    observations.append(observation)\n",
    "    # get info from environment\n",
    "    observation, reward, terminated, truncated, info = env_2.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # end of episode\n",
    "    if terminated or truncated:\n",
    "        observations.append(observation)\n",
    "        ep_length = len(observations[:-1]) # Do not take the terminal state as we have no action in the terminal state\n",
    "        ep_total_reward = np.sum(np.array(rewards))\n",
    "        episodes_total_rewards.append(ep_total_reward)\n",
    "        returns = np.zeros(len(observations) - 1)\n",
    "        for timestep in reversed(range(ep_length)):\n",
    "\n",
    "            # calculate state's actual return by looking at reward + future rewards\n",
    "            terminal = timestep == len(rewards) - 1\n",
    "            returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actual_state_util = torch.zeros((len(returns), 1), device=device)\n",
    "            for i, actual_util in enumerate(returns):\n",
    "                actual_state_util[i] = torch.tensor(returns[i], device=device)\n",
    "            # calculate baseline expected state value\n",
    "            input_state_util = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            for i, input_samples in enumerate(observations[:-1]):\n",
    "                input_state_util[i] = torch.tensor(observations[i], device=device)\n",
    "        pred_state_util = state_util_model(input_state_util)\n",
    "        loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "        \n",
    "        # some extra info helpful for debug\n",
    "        with torch.no_grad():\n",
    "            state_pred_err = np.abs(loss_state_utility.detach().clone().mean().item())\n",
    "            state_preds.append(pred_state_util.detach().clone())\n",
    "            state_err.append(state_pred_err)\n",
    "            state_util_differences = []\n",
    "            for timestep in range(ep_length):\n",
    "                # make updates to policy (specific action) based on return\n",
    "                # get the state's return minus the baseline (predicted state return)\n",
    "                state_util_differences.append(actual_state_util.detach().clone()[timestep] - pred_state_util.detach().clone()[timestep])\n",
    "        \n",
    "            # TODO: change to store rather than recomputation, but without autograd complaining about inplace operations\n",
    "            # e.g. putting the tensor in a list\n",
    "        with torch.no_grad():\n",
    "            input_policy = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            for i, input_samples in enumerate(observations[:-1]):\n",
    "                input_policy[i] = torch.tensor(observations[i], device=device)\n",
    "            actions_chosen_tensor = torch.zeros((len(action_indices), num_of_actions), device=device)\n",
    "            for i, action_index in enumerate(action_indices):\n",
    "                actions_chosen_tensor[i][action_index] = 1\n",
    "            state_util_diffs_tensor = torch.tensor(state_util_differences, device=device)\n",
    "        recomputed_policy = policy_model(input_state_util)\n",
    "        loss_policy = policy_loss(recomputed_policy, actions_chosen_tensor, state_util_diffs_tensor)\n",
    "\n",
    "        episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "        # accumulate, avg, and add gradients to parameters for state value network\n",
    "        loss_state_utility.sum().backward()\n",
    "        state_util_adam.step()\n",
    "        state_util_adam.zero_grad()\n",
    "    \n",
    "        # accumulate, avg, and add gradients to parameters for policy network\n",
    "        loss_policy.sum().backward()\n",
    "        policy_adam.step()\n",
    "        policy_adam.zero_grad()\n",
    "\n",
    "        observation, info = env_2.reset()\n",
    "        observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "        state_err, state_preds = [], []\n",
    "\n",
    "\n",
    "env_2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(gym.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00c7148fe7d049885671e82bbf6f02dbbdff16ff92bf68e1f2741c72b6e7373b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
