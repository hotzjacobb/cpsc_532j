{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Assignment 2\n",
    "\n",
    "Instructions: Implement both PG and an evolutionary algorithm to solve the Open AI Gym Lunar Lander problem, and then apply it to my area of choice, which is chess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to do some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"gpu\" # ðŸ§®\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = \"mps\" # ðŸ§ \n",
    "else:\n",
    "    device = \"cpu\" # ðŸ¥º\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to write the code for our Policy Gradient function with a baseline (REINFORCE). I'm going to use PyTorch as my neural network library.\n",
    "\n",
    "I'm going to start with a basic feed forward net for both the network that chooses the policy and the network that learns states' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the policy network for choosing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PolicyChoice(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyChoice, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4), # TODO: try reducing to one hidden layer if learning proves initially dificult\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Softmax(dim=0) # log softmax for a nice interpretation as probabilities of choosing actions\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs = self.policy(x)\n",
    "        return probs\n",
    "\n",
    "policy_model = PolicyChoice().to(device)\n",
    "policy_adam = torch.optim.Adam(policy_model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our loss function for the policy network, we want to adjust just the parameters with the primary aim of affecting the probability of taking the action that we took on that time step. If the return of the resulting state is better than expected, we want to increase it proportionally. If it is less than expected, we want to decrease it proportionally. Thus, we multiply the gradient of the parameter weights w.r.t. the taken action's probability by the difference of the return for that state-action pair.\n",
    "\n",
    "Importantly, there is an extra factor however that we must consider; when we decide that we want to take the gradient of the parameters w.r.t. a specific action's return, the policy expectancy must be multiplied by the specific action's likelihood to determine the value it contributes to the policy. Thus, we end up with the gradient of the action's probability conditioned on the state and parameters. \n",
    "\n",
    "Thus, the general concept of loss to backpropogate in the REINFORCE algorithm is:\n",
    "\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\frac{\\nabla\\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)}$\n",
    "\n",
    "This can be expressed as:\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\nabla \\ln{\\pi(A_t|S_t, \\theta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below just worries about the loss and not the gradient, as PyTorch provides autograd differntiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(prob, state_util_difference):\n",
    "    nll_loss = nn.NLLLoss()\n",
    "    return nll_loss(prob, torch.ones(1)) * state_util_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the network for approximating state utililities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateUtility(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StateUtility, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.state_utility = nn.Sequential(\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4), # TODO: try reducing to one hidden layer if learning proves initially dificult\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1), # output a tensor of a scalar value\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        state_utility = self.state_utility(x)\n",
    "        return state_utility\n",
    "\n",
    "state_util_model = StateUtility().to(device)\n",
    "state_util_adam = torch.optim.Adam(state_util_model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the state utilities network, we just use L1 loss with the gradients of W with respect to state utility.\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}(S_t, W)) \\nabla \\hat{\\upsilon}(S_t, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, the code below just worries about the loss and not the gradient, as PyTorch provides autograd differntiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_util_loss(calculated_state_value, episode_state_value):\n",
    "    # the overall state value is the input, and the individual state value is our target\n",
    "    l1_loss = nn.L1Loss()\n",
    "    return l1_loss(calculated_state_value, episode_state_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to calculate a state instance's utility in a given episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ep_state_util():\n",
    "    \"\"\"Given an observation and reward from an\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the Lunar Lander environment now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-350.86130554663714, -189.8865101089612, -103.03847412831941, -581.2043445816703, -163.93252346597382, -49.51899536481143, -260.75239283003975, -233.18472395477846, -466.09337063747716, -274.9756752804017, -348.73756389432504, -595.4321930174558, -387.43727949568694, -477.39161292267676, -507.30093546356005, -162.46922987107797, -480.7745698076816, -409.8684494245649, -7.226011881097378, -336.07919890640403, -564.960104218054, -181.96143509468413, -473.21099556072414, -479.99705740041173, -96.61613820825796, -491.8673565150349, -289.54653009657926, -430.16777627322404, -89.96845138200774, -178.35957658831273, -251.79772720771726, -491.3091930132326, 11.087863045058057, -64.56443510680057, -175.7370180208552, -101.25521031004382, -288.41384107254646, -318.2153006563705, -334.0488297946413, -101.04652557200455, -79.15067410829134, -373.81468585665124, -86.61116853649762, -357.5281151310988, -234.93475040137642, -294.79145035570787, -230.65786723327614, -520.8971483584046, -75.81210897889616, -573.214647168107, -231.2539655957972, -94.94036853678487, -548.9558528512259, -265.31442310382266, -398.52401588647234, -340.1992555263682, -577.8458528512951, -442.53257219703096, -414.02708459711516, -500.08115228951254, -378.993838431245, -450.63030310553137, -265.80670371587, -447.9041199619491, -531.9143855643335, -164.53616851860477, -333.16414556010966, -54.885298908642326, -591.6272303315953, -459.65848282724306, -536.6195323436419, -287.9055253098926, -402.59364959090556, -547.3663135734687, -147.12548113835743, -496.40781509170864, -180.7511079688093, -489.8429636975902, -321.3466713752259, -275.3823488128247, -455.19339992837206, -565.6487659608582, -486.5629475974856, -489.712699674003, -507.8551052853507, -190.76006135106053, -336.3645461363688, -378.53411085086327, -78.14874297888215, -437.5929087581171, -93.33219867883814, -533.0931035737372, -223.68356893035806, -473.9153225348286, -433.3412355642476, -26.46083030506152, -333.8850688323463, -403.40840380984855, -477.6899807176833]\n"
     ]
    }
   ],
   "source": [
    "# TODO: use a custom dataloader class and see if speed up\n",
    "\n",
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    #render_mode=\"human\"\n",
    ")\n",
    "\n",
    "action_space_seed = np.random.seed(13)\n",
    "\n",
    "observation, info = env.reset(seed=13)\n",
    "\n",
    "# index i in the lists below corresponds to the timestep i of the current episode\n",
    "observations = []\n",
    "rewards = []\n",
    "episode_total_rewards = []\n",
    "\n",
    "for timestep in range(10000):\n",
    "    action_weights = np.array(policy_model(torch.tensor(observation)).tolist())\n",
    "    action_array = np.random.multinomial(n=1, pvals=action_weights)\n",
    "    action = np.argmax(action_array)\n",
    "    \n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    observations.append(observation)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # end of episode\n",
    "    if terminated or truncated:\n",
    "        ep_length = len(observations)\n",
    "        ep_total_reward = np.sum(np.array(rewards))\n",
    "        episode_total_rewards.append(ep_total_reward)\n",
    "        for timestep in reversed(range(ep_length)):\n",
    "\n",
    "            terminal = timestep == len(rewards) - 1\n",
    "            state_utilities = np.zeros(len(observations))\n",
    "            state_utilities[timestep] = rewards[timestep] + (gamma * state_utilities[timestep+1]) if not terminal else rewards[timestep]\n",
    "            \n",
    "            pred_state_util = state_util_model(torch.tensor(observations[timestep]))\n",
    "            actual_state_util = torch.tensor([state_utilities[timestep]])\n",
    "            \n",
    "            loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "            \n",
    "            state_util_adam.zero_grad()\n",
    "            loss_state_utility.backward()\n",
    "            state_util_adam.step()\n",
    "\n",
    "        observation, info = env.reset()\n",
    "        observations, rewards = [], []\n",
    "\n",
    "print(episode_total_rewards)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00c7148fe7d049885671e82bbf6f02dbbdff16ff92bf68e1f2741c72b6e7373b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
