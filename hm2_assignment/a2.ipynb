{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Instructions: Implement both PG and an evolutionary algorithm to solve the Open AI Gym Lunar Lander problem, and then apply it to my area of choice, which is chess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to do some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "\n",
    "seed = 245\n",
    "action_space_seed = np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to write the code for our Policy Gradient function with a baseline (taken from REINFORCE). I'm going to use PyTorch as my neural network library (I want to try JAX, but this is the more practical choice for me at the moment. Exploration-Exploitation tradeoff ü§∑‚Äç‚ôÇÔ∏è).\n",
    "\n",
    "I'm going to start with a basic feed forward net for both the network that chooses the policy and the network that learns states' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the policy network for choosing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PolicyChoice(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyChoice, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(8, 32)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(32, 4)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_weighted1 = self.layer1(x)\n",
    "        h1 = self.relu1(x_weighted1)\n",
    "        x_weighted2 = self.layer2(h1)\n",
    "        h2 = self.relu2(x_weighted2)\n",
    "        logits = self.layer3(h2)\n",
    "        return logits\n",
    "\n",
    "policy_model = PolicyChoice().to(device)\n",
    "# RL convention is grad. ascent I think\n",
    "policy_adam = torch.optim.Adam(params=policy_model.parameters(), lr=1, maximize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our loss function for the policy network, we want to adjust just the parameters with the primary aim of affecting the probability of taking the action that we took on that time step. If the return of the resulting state is better than expected, we want to increase it proportionally. If it is less than expected, we want to decrease it proportionally. Thus, we multiply the gradient of the parameter weights w.r.t. the taken action's probability by the difference of the return for that state-action pair.\n",
    "\n",
    "Importantly, there is an extra factor however that we must consider; when we decide that we want to take the gradient of the parameters w.r.t. a specific action's return, the policy expectancy must be multiplied by the specific action's likelihood to determine the value it contributes to the policy. Thus, we end up with the gradient of the action's probability conditioned on the state and parameters. \n",
    "\n",
    "Thus, the general concept of loss to backpropogate in the REINFORCE algorithm is:\n",
    "\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\frac{\\nabla\\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)}$\n",
    "\n",
    "This can be expressed as:\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\nabla \\ln{\\pi(A_t|S_t, \\theta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below just worries about the loss and not the gradient, as PyTorch provides autograd differentiation behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_score(logits, action_chosen, state_util_difference):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    # NOTE: I think RL literature typically describes problems as gradient ascent\n",
    "    # thus this is a score function\n",
    "    action_scaled = torch.mul(ce_loss(logits, action_chosen), torch.unsqueeze(state_util_difference, dim=1))\n",
    "    return action_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the network for approximating state utililities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateUtility(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StateUtility, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(8, 64)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(64, 2)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.layer3 = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_weighted = self.layer1(x)\n",
    "        h1 = self.relu1(x_weighted)\n",
    "        h1_weighted = self.layer2(h1)\n",
    "        h2 = self.relu2(h1_weighted)\n",
    "        state_utility = self.layer3(h2)\n",
    "        return state_utility\n",
    "\n",
    "state_util_model = StateUtility().to(device)\n",
    "state_util_adam = torch.optim.Adam(params=state_util_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the state utilities network, we just use L1 loss with the gradients of W with respect to state utility.\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}(S_t, W)) \\nabla \\hat{\\upsilon}(S_t, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, the code below just worries about the loss and not the gradient, as PyTorch provides autograd differntiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_util_loss(calculated_state_value, episode_state_value):\n",
    "    # the overall state value is the input, and the individual state value is our target\n",
    "    l1_loss = nn.L1Loss(reduction='none')\n",
    "    return l1_loss(calculated_state_value, episode_state_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I've chosen some hyperparameters: L1 Loss for the state utiltily network. My intuition when setting out was that maybe the L2 loss needs more data to fit, whereas absolute value doesn't. I'm not really sure what the best choice is here (maybe actually L2 for robustness) and having a more conservative baseline early is probably better, but I stuck with L1 nevertheless.\n",
    "\n",
    "I played around with learning rates, which of course need to change if we change things like gamma or the loss function, and empirically these are the lr's that I got my best result with.\n",
    "\n",
    "Let's now set Gamma (our discount factor for future states' returns) below. This was also the best one for me empirically. In my paper writeup, I discuss how having Gamma much too low dor a long time caused me a lot of headache. I do have a better explanation though for choosing an appropriate Gamma and why this works. The reward for solving/not solving it +/- 100 in the terminal state. After some discussion, I realized my Gamma was far too low and I confirmed it by imagining a long episode of 150 timesteps. In this episode, anything below .98 was less than the average return that I was calculating for the first steps. Thus, if this future return that is a signal of ultimate success, the initial states can not plan for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the Lunar Lander environment now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    ")\n",
    "\n",
    "# just for visualization\n",
    "env_2 = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    render_mode=\"human\",\n",
    "    \n",
    ")\n",
    "\n",
    "num_of_actions = 4\n",
    "\n",
    "gym_seed = seed\n",
    "\n",
    "observation, info = env.reset(seed=gym_seed)\n",
    "\n",
    "episodes_total_rewards_sums = []\n",
    "# for debug of state-value funtion\n",
    "episode_total_state_err = []\n",
    "\n",
    "observations = []\n",
    "# NOTE: rewards[0] corresponds to the result of calc_reward(state_of(observations[0]), action_indices[0])\n",
    "# thus len(rewards) == len(action_indices) == len(observations) - 1\n",
    "# i.e. no reward for the first timestep, no action_index for the last timestep\n",
    "rewards = []\n",
    "action_indices = []\n",
    "action_logits_per_ep = []\n",
    "state_preds = []\n",
    "state_err = []\n",
    "\n",
    "policy_adam.zero_grad()\n",
    "state_util_adam.zero_grad()\n",
    "\n",
    "\n",
    "episodes_total_rewards_sums = []\n",
    "ep_total_rewards = []\n",
    "# for debug of state-value funtion\n",
    "episode_total_state_err = []\n",
    "action_logits_episodes = []\n",
    "observations, rewards, returns, action_indices, action_logits_per_ep = [], [], [], [], []\n",
    "state_err, state_preds = [], []\n",
    "\n",
    "\n",
    "# everything unfrozen\n",
    "for timestep in range(1000000):\n",
    "    \n",
    "    # use policy gradient to get action probabilities; sample stochastically\n",
    "    action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "    with torch.no_grad():\n",
    "        action_logits_per_ep.append(action_logits.detach().clone())\n",
    "        action_probs = torch.nn.functional.softmax(action_logits, dim=0)\n",
    "        action_sampling = torch.multinomial(action_probs, 1)\n",
    "        action = action_sampling.item()\n",
    "        action_indices.append(action)\n",
    "    \n",
    "    observations.append(observation)\n",
    "    # get info from environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # end of episode\n",
    "    if terminated or truncated:\n",
    "        observations.append(observation)\n",
    "        ep_length = len(observations[:-1]) # Do not take the terminal state as we have no action in the terminal state\n",
    "        ep_total_rewards_sum = np.sum(np.array(rewards))\n",
    "        ep_total_rewards.append(rewards)\n",
    "        episodes_total_rewards_sums.append(ep_total_rewards_sum)\n",
    "        returns = np.zeros(len(observations) - 1)\n",
    "        for timestep in reversed(range(ep_length)):\n",
    "\n",
    "            # calculate state's actual return by looking at reward + future rewards\n",
    "            terminal = timestep == len(rewards) - 1\n",
    "            returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actual_state_util = torch.zeros((len(returns), 1), device=device)\n",
    "            for i, actual_util in enumerate(returns):\n",
    "                actual_state_util[i] = torch.tensor(returns[i], device=device)\n",
    "            # calculate baseline expected state value\n",
    "            input_state_util = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            for i, input_samples in enumerate(observations[:-1]):\n",
    "                input_state_util[i] = torch.tensor(observations[i], device=device)\n",
    "        pred_state_util = state_util_model(input_state_util)\n",
    "        loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "        \n",
    "        # some extra info helpful for debug\n",
    "        with torch.no_grad():\n",
    "            state_pred_err = np.abs(loss_state_utility.detach().clone().mean().item())\n",
    "            state_preds.append(pred_state_util.detach().clone())\n",
    "            state_err.append(state_pred_err)\n",
    "            state_util_differences = []\n",
    "            for timestep in range(ep_length):\n",
    "                # make updates to policy (specific action) based on return\n",
    "                # get the state's return minus the baseline (predicted state return)\n",
    "                state_util_differences.append(actual_state_util.detach().clone()[timestep] - pred_state_util.detach().clone()[timestep])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_policy = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            for i, input_samples in enumerate(observations[:-1]):\n",
    "                input_policy[i] = torch.tensor(observations[i], device=device)\n",
    "            actions_chosen_tensor = torch.zeros((len(action_indices), num_of_actions), device=device)\n",
    "            for i, action_index in enumerate(action_indices):\n",
    "                actions_chosen_tensor[i][action_index] = 1\n",
    "            state_util_diffs_tensor = torch.tensor(state_util_differences, device=device)\n",
    "        recomputed_policy = policy_model(input_state_util)\n",
    "        loss_policy = policy_score(recomputed_policy, actions_chosen_tensor, state_util_diffs_tensor)\n",
    "\n",
    "        episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "        # add gradients to parameters for state value network\n",
    "        loss_state_utility.sum().backward()\n",
    "        state_util_adam.step()\n",
    "        state_util_adam.zero_grad()\n",
    "    \n",
    "        # add gradients to parameters for policy network\n",
    "        loss_policy.sum().backward()\n",
    "        policy_adam.step()\n",
    "        policy_adam.zero_grad()\n",
    "\n",
    "        gym_seed += 1\n",
    "        observation, info = env.reset(seed=gym_seed)\n",
    "        action_logits_episodes.append(action_logits_per_ep)\n",
    "        observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "        state_err, state_preds = [], []\n",
    "\n",
    "print(f'The avg. state val prediction error on the first quarter of episodes was: {np.sum(episode_total_state_err[:len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the second quarter of episodes was: {np.sum(episode_total_state_err[len(episode_total_state_err)//4:2 * len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the third quarter of episodes was: {np.sum(episode_total_state_err[2 * len(episode_total_state_err)//4:3 *len(episode_total_state_err)//4]) / (len(episode_total_state_err)/4)}')\n",
    "print(f'The avg. state val prediction error on the fourth quarter of episodes was: {np.sum(episode_total_state_err[3 *len(episode_total_state_err)//4:len(episode_total_state_err)]) / (len(episode_total_state_err)/4)}')\n",
    "\n",
    "print(f'The avg. episode reward on the first quarter of episodes was: {np.sum(episodes_total_rewards_sums[:len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)//4)}')\n",
    "print(f'The avg. episode reward on the second quarter of episodes was: {np.sum(episodes_total_rewards_sums[len(episodes_total_rewards_sums)//4:2 * len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "print(f'The avg. episode reward on the third quarter of episodes was: {np.sum(episodes_total_rewards_sums[2 * len(episodes_total_rewards_sums)//4:3 *len(episodes_total_rewards_sums)//4]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "print(f'The avg. episode reward on the fourth quarter of episodes was: {np.sum(episodes_total_rewards_sums[3 *len(episodes_total_rewards_sums)//4:len(episodes_total_rewards_sums)]) / (len(episodes_total_rewards_sums)/4)}')\n",
    "print(f'The avg. episode reward on the last 100 episodes was: {np.sum(episodes_total_rewards_sums[-100:]) / (100)}')\n",
    "\n",
    "\n",
    "# Same as above; just to see what the model has learnt visually\n",
    "gym_seed += 1\n",
    "observation, info = env_2.reset(seed=gym_seed)\n",
    "\n",
    "observations, rewards, returns, action_indices, action_logits_per_ep = [], [], [], [], []\n",
    "\n",
    "episodes_total_rewards = []\n",
    "\n",
    "# just to see what the model has learnt visually\n",
    "for timestep in range(1000000):\n",
    "    \n",
    "    # use policy gradient to get action probabilities; sample stochastically\n",
    "    action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "    with torch.no_grad():\n",
    "        action_logits_per_ep.append(action_logits.detach().clone())\n",
    "        action_probs = torch.nn.functional.softmax(action_logits, dim=0)\n",
    "        action_sampling = torch.multinomial(action_probs, 1)\n",
    "        action = action_sampling.item()\n",
    "        action_indices.append(action)\n",
    "    \n",
    "    observations.append(observation)\n",
    "    # get info from environment\n",
    "    observation, reward, terminated, truncated, info = env_2.step(action)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # end of episode\n",
    "    if terminated or truncated:\n",
    "        observations.append(observation)\n",
    "        ep_length = len(observations[:-1]) # Do not take the terminal state as we have no action in the terminal state\n",
    "        ep_total_reward = np.sum(np.array(rewards))\n",
    "        episodes_total_rewards.append(ep_total_reward)\n",
    "        returns = np.zeros(len(observations) - 1)\n",
    "        for timestep in reversed(range(ep_length)):\n",
    "\n",
    "            # calculate state's actual return by looking at reward + future rewards\n",
    "            terminal = timestep == len(rewards) - 1\n",
    "            returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            actual_state_util = torch.zeros((len(returns), 1), device=device)\n",
    "            for i, actual_util in enumerate(returns):\n",
    "                actual_state_util[i] = torch.tensor(returns[i], device=device)\n",
    "            # calculate baseline expected state value\n",
    "            input_state_util = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            for i, input_samples in enumerate(observations[:-1]):\n",
    "                input_state_util[i] = torch.tensor(observations[i], device=device)\n",
    "        pred_state_util = state_util_model(input_state_util)\n",
    "        loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "        \n",
    "        # some extra info helpful for debug\n",
    "        with torch.no_grad():\n",
    "            state_pred_err = np.abs(loss_state_utility.detach().clone().mean().item())\n",
    "            state_preds.append(pred_state_util.detach().clone())\n",
    "            state_err.append(state_pred_err)\n",
    "            state_util_differences = []\n",
    "            for timestep in range(ep_length):\n",
    "                # make updates to policy (specific action) based on return\n",
    "                # get the state's return minus the baseline (predicted state return)\n",
    "                state_util_differences.append(actual_state_util.detach().clone()[timestep] - pred_state_util.detach().clone()[timestep])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_policy = torch.zeros((len(observations)-1, len(observation)), device=device)\n",
    "            for i, input_samples in enumerate(observations[:-1]):\n",
    "                input_policy[i] = torch.tensor(observations[i], device=device)\n",
    "            actions_chosen_tensor = torch.zeros((len(action_indices), num_of_actions), device=device)\n",
    "            for i, action_index in enumerate(action_indices):\n",
    "                actions_chosen_tensor[i][action_index] = 1\n",
    "            state_util_diffs_tensor = torch.tensor(state_util_differences, device=device)\n",
    "        recomputed_policy = policy_model(input_state_util)\n",
    "        loss_policy = policy_score(recomputed_policy, actions_chosen_tensor, state_util_diffs_tensor)\n",
    "\n",
    "        episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "        # add gradients to parameters for state value network\n",
    "        loss_state_utility.sum().backward()\n",
    "        state_util_adam.step()\n",
    "        state_util_adam.zero_grad()\n",
    "    \n",
    "        # add gradients to parameters for policy network\n",
    "        loss_policy.sum().backward()\n",
    "        policy_adam.step()\n",
    "        policy_adam.zero_grad()\n",
    "\n",
    "\n",
    "        gym_seed += 1\n",
    "        observation, info = env_2.reset(seed=gym_seed)\n",
    "        observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "        state_err, state_preds = [], []\n",
    "\n",
    "\n",
    "env_2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, we learned something! (We learned to turn all engines off and hope that we land right side up and close to the centre) We're stuck in a local optimum in the search space. Though I definitely didn't solve this problem unfortunately. I had a few small but key misconceptions when starting (point taken for me to read the documentation more carefully) and maybe could have reached a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decide our population size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use direct encoding to keep things simple, although indirect encoding allows for more possibilities (and compression) and seemed to be more common in recent literature.\n",
    "\n",
    "There are also options in terms of what a member of the population is: We can use a neural net to be able to approximate any behaviour in theory, however that has some complexities in terms of crossover as network weights are not independent. Something more ambitious that delves into network architecture like NEAT or HYPERNEAT is a non-starter.\n",
    "\n",
    "Thus I'm going to define a population member simply here to try not to go over scope like I did in the prior question debugging. Thus, I'm going to use a really simple definition. I could for example, do something in between, that's a behaviour space parameter (instead of jus taction space params) like tendency to fire enginer when x-coordinate < some_number, but I'll do this simpler case to illustrate GA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, do_nothing_prob, left_eng_prob, main_eng_prob, right_eng_prob):\n",
    "        self.do_nothing_prob = do_nothing_prob\n",
    "        self.left_eng_prob = left_eng_prob\n",
    "        self.main_eng_prob = main_eng_prob\n",
    "        self.right_eng_prob = right_eng_prob\n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00c7148fe7d049885671e82bbf6f02dbbdff16ff92bf68e1f2741c72b6e7373b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
