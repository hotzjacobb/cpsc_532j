{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Instructions: Implement both PG and an evolutionary algorithm to solve the Open AI Gym Lunar Lander problem, and then apply it to my area of choice, which is chess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to do some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"gpu\" # üßÆ\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = \"mps\" # üß†\n",
    "else:\n",
    "    device = \"cpu\" # ü•∫\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to write the code for our Policy Gradient function with a baseline (taken from REINFORCE). I'm going to use PyTorch as my neural network library (I want to try JAX, but this is the more practical choice for me at the moment. Exploration-Exploitation tradeoff ü§∑‚Äç‚ôÇÔ∏è).\n",
    "\n",
    "I'm going to start with a basic feed forward net for both the network that chooses the policy and the network that learns states' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the policy network for choosing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PolicyChoice(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyChoice, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4) # logits; PyTorch built-in loss applies softmax later\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs = self.policy(x)\n",
    "        return probs\n",
    "\n",
    "policy_model = PolicyChoice().to(device)\n",
    "policy_adam = torch.optim.Adam(policy_model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our loss function for the policy network, we want to adjust just the parameters with the primary aim of affecting the probability of taking the action that we took on that time step. If the return of the resulting state is better than expected, we want to increase it proportionally. If it is less than expected, we want to decrease it proportionally. Thus, we multiply the gradient of the parameter weights w.r.t. the taken action's probability by the difference of the return for that state-action pair.\n",
    "\n",
    "Importantly, there is an extra factor however that we must consider; when we decide that we want to take the gradient of the parameters w.r.t. a specific action's return, the policy expectancy must be multiplied by the specific action's likelihood to determine the value it contributes to the policy. Thus, we end up with the gradient of the action's probability conditioned on the state and parameters. \n",
    "\n",
    "Thus, the general concept of loss to backpropogate in the REINFORCE algorithm is:\n",
    "\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\frac{\\nabla\\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)}$\n",
    "\n",
    "This can be expressed as:\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\nabla \\ln{\\pi(A_t|S_t, \\theta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below just worries about the loss and not the gradient, as PyTorch provides autograd differentiation behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(logits, state_util_difference, action_idx):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    actions_tensor = torch.zeros(4)\n",
    "    actions_tensor[action_idx] = 1\n",
    "    return ce_loss(logits, actions_tensor) * state_util_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the network for approximating state utililities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateUtility(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StateUtility, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.state_utility = nn.Sequential(\n",
    "            # nn.Linear(8, 4),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(4, 2), # TODO: try reducing to one hidden layer if learning proves initially dificult\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(2, 1), # output a tensor of a scalar value\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1), # output a tensor of a scalar value\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        state_utility = self.state_utility(x)\n",
    "        return state_utility\n",
    "\n",
    "state_util_model = StateUtility().to(device)\n",
    "state_util_adam = torch.optim.Adam(state_util_model.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the state utilities network, we just use L1 loss with the gradients of W with respect to state utility.\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}(S_t, W)) \\nabla \\hat{\\upsilon}(S_t, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, the code below just worries about the loss and not the gradient, as PyTorch provides autograd differntiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_util_loss(calculated_state_value, episode_state_value):\n",
    "    # the overall state value is the input, and the individual state value is our target\n",
    "    l1_loss = nn.L1Loss()\n",
    "    return l1_loss(calculated_state_value, episode_state_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having limitied compute (and longer runtimes) led to me to reflect on gamma tuning. I initially noticed that having a higher discount factor (smaller gamma) improved values a lot for this Lunar Lander task. My first thought was that it's a reflection of the fact that this task has very well-defined rewards that are frequent and that reflect short-term actions.\n",
    "\n",
    "That's definitely true, but as I reflected on it, it also occurred to me that using a higher discount factor is, in general, a tradeoff. With a higher discount factor you are hamstringing your ability to learn long-term dependencies, but you can learn action's values much faster. However with a very low discount factor you can actually still theoretically learn actions fine grained values and not conflate them, but it just takes a lot more training examples as the Monte Carlo nature of sampling will eventually lead to distinction (my models with higher gammas seemed to still be learning when they terminated). And with this lower factor you can also learn long-term dependencies fairly easy.\n",
    "\n",
    "I interpreted this as meaning that I should leave gamma a little higher than it's best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .4\n",
    "\n",
    "# gamma = .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the Lunar Lander environment now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use a custom dataloader class and see if speed up\n",
    "\n",
    "env = gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    #render_mode=\"human\"\n",
    ")\n",
    "\n",
    "action_space_seed = np.random.seed(13)\n",
    "\n",
    "observation, info = env.reset(seed=13)\n",
    "\n",
    "episodes_total_rewards = []\n",
    "# for debug of state-value funtion\n",
    "episode_total_state_err = []\n",
    "\n",
    "# index i in the lists below corresponds to the timestep i of the current episode\n",
    "observations = []\n",
    "rewards = []\n",
    "action_indices = []\n",
    "action_logits_per_ep = []\n",
    "# for debug of state-value funtion\n",
    "state_err = []\n",
    "\n",
    "policy_adam.zero_grad()\n",
    "state_util_adam.zero_grad()\n",
    "\n",
    "\n",
    "#for timestep in range(1000000):\n",
    "for timestep in range(100000):\n",
    "\n",
    "    if timestep==0 or timestep==30000:\n",
    "        print('debug entry')\n",
    "    \n",
    "    # use policy gradient to get action probabilities; sample stochastically\n",
    "    action_logits = policy_model(torch.tensor(observation, device=device, dtype=torch.float32))\n",
    "    with torch.no_grad():\n",
    "        action_logits_per_ep.append(action_logits.detach().clone())\n",
    "        action_probs = np.array(torch.nn.functional.softmax(action_logits, dim=0)).tolist()\n",
    "        action_sampling = np.random.multinomial(n=1, pvals=action_probs)\n",
    "        action = np.argmax(action_sampling)\n",
    "        action_indices.append(action)\n",
    "    \n",
    "    # get info from environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    observations.append(observation)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    # end of episode\n",
    "    if terminated or truncated:\n",
    "        ep_length = len(observations)\n",
    "        ep_total_reward = np.sum(np.array(rewards))\n",
    "        episodes_total_rewards.append(ep_total_reward)\n",
    "        returns = np.zeros(len(observations))\n",
    "        for timestep in reversed(range(ep_length)):\n",
    "\n",
    "            # calculate state's actual return by looking at reward + future rewards\n",
    "            terminal = timestep == len(rewards) - 1\n",
    "            returns[timestep] = rewards[timestep] + (gamma * returns[timestep+1]) if not terminal else rewards[timestep]\n",
    "            # calculate baseline expected state value\n",
    "            pred_state_util = state_util_model(torch.tensor(observations[timestep], device=device, dtype=torch.float32))\n",
    "            actual_state_util = torch.tensor([returns[timestep]], device=device, dtype=torch.float32)\n",
    "            loss_state_utility = state_util_loss(pred_state_util, actual_state_util)\n",
    "            with torch.no_grad():\n",
    "                state_pred_err = np.abs(loss_state_utility.item())\n",
    "                state_err.append(state_pred_err)\n",
    "\n",
    "            # update state utility function\n",
    "            loss_state_utility.backward()\n",
    "\n",
    "            # make updates to policy (specific action) based on return\n",
    "            with torch.no_grad():\n",
    "                # get the state's return minus the basline (predicted state return)\n",
    "                state_util_difference = actual_state_util - pred_state_util\n",
    "            # loss_policy = policy_loss(action_logits_per_ep[timestep], state_util_difference, action_indices[timestep])\n",
    "            # TODO: change to store rather than recomputation, but without autograd complaining about inplace operations\n",
    "            loss_policy = policy_loss(policy_model(torch.tensor(observation, device=device, dtype=torch.float32)), state_util_difference, action_indices[timestep])\n",
    "\n",
    "            # update policy gradients\n",
    "            loss_policy.backward()\n",
    "\n",
    "        episode_total_state_err.append(np.sum(np.array(state_err)))\n",
    "\n",
    "        # sum gradients for state value parameters\n",
    "        state_util_adam.step()\n",
    "        state_util_adam.zero_grad()\n",
    "\n",
    "        # perform policy network update\n",
    "        policy_adam.step()\n",
    "        policy_adam.zero_grad()\n",
    "\n",
    "        observation, info = env.reset()\n",
    "        observations, rewards, action_indices, action_logits_per_ep = [], [], [], []\n",
    "        state_err = []\n",
    "\n",
    "# TODO: move these to a self-contained function\n",
    "\n",
    "print(f'The avg. state val prediction error on the first quarter of episodes was: {np.sum(episode_total_state_err[:len(episode_total_state_err)//4]) / len(episode_total_state_err)//4}')\n",
    "print(f'The avg. state val prediction error on the second quarter of episodes was: {np.sum(episode_total_state_err[len(episode_total_state_err)//4:2 * len(episode_total_state_err)//4]) / len(episode_total_state_err)//4}')\n",
    "print(f'The avg. state val prediction error on the third quarter of episodes was: {np.sum(episode_total_state_err[2 * len(episode_total_state_err)//4:3 *len(episode_total_state_err)//4]) / len(episode_total_state_err)//4}')\n",
    "print(f'The avg. state val prediction error on the fourth quarter of episodes was: {np.sum(episode_total_state_err[3 *len(episode_total_state_err)//4:len(episode_total_state_err)]) / len(episode_total_state_err)//4}')\n",
    "\n",
    "print(f'The avg. episode reward on the first quarter of episodes was: {np.sum(episodes_total_rewards[:len(episodes_total_rewards)//4]) / len(episodes_total_rewards)//4}')\n",
    "print(f'The avg. episode reward on the second quarter of episodes was: {np.sum(episodes_total_rewards[len(episodes_total_rewards)//4:2 * len(episodes_total_rewards)//4]) / len(episodes_total_rewards)//4}')\n",
    "print(f'The avg. episode reward on the third quarter of episodes was: {np.sum(episodes_total_rewards[2 * len(episodes_total_rewards)//4:3 *len(episodes_total_rewards)//4]) / len(episodes_total_rewards)//4}')\n",
    "print(f'The avg. episode reward on the fourth quarter of episodes was: {np.sum(episodes_total_rewards[3 *len(episodes_total_rewards)//4:len(episodes_total_rewards)]) / len(episodes_total_rewards)//4}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(gym.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00c7148fe7d049885671e82bbf6f02dbbdff16ff92bf68e1f2741c72b6e7373b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
