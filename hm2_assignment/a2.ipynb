{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Assignment 2\n",
    "\n",
    "Instructions: Implement both PG and an evolutionary algorithm to solve the Open AI Gym Lunar Lander problem, and then apply it to my area of choice, which is chess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to do some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"gpu\" # ðŸ§®\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = \"mps\" # ðŸ§ \n",
    "else:\n",
    "    device = \"cpu\" # ðŸ¥º\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to write the code for our Policy Gradient function with a baseline (REINFORCE). I'm going to use PyTorch as my neural network library.\n",
    "\n",
    "I'm going to start with a basic feed forward net for both the network that chooses the policy and the network that learns states' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the policy network for choosing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class PolicyChoice(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyChoice, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4), # TODO: try reducing to one hidden layer if learning proves initially dificult\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 4),\n",
    "            nn.Softmax() # log softmax for a nice interpretation as probabilities of choosing actions\n",
    "        )\n",
    "        # self.first_lin = nn.Linear(8, 8)\n",
    "        # self.first_relu = nn.ReLU()\n",
    "        # self.second_lin = nn.Linear(8, 4) # TODO: try reducing to one hidden layer if learning proves initially dificult\n",
    "        # self.second_relu = nn.ReLU()\n",
    "        # self.third_lin = nn.Linear(4, 4)\n",
    "        # self.soft_max = nn.Softmax(dim=0) # log softmax for a nice interpretation as probabilities of choosing actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs = self.policy(x)\n",
    "        # x = self.first_lin(x)\n",
    "        # x = self.first_relu(x)\n",
    "        # x = self.second_lin(x)\n",
    "        # x = self.second_relu(x)\n",
    "        # x = self.third_lin(x)\n",
    "        # x = self.soft_max(x)\n",
    "        # return x\n",
    "        return probs\n",
    "\n",
    "policy_model = PolicyChoice().to(device)\n",
    "policy_adam = torch.optim.Adam(policy_model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our loss function for the policy network, we want to adjust just the parameters with the primary aim of affecting the probability of taking the action that we took on that time step. If the return of the resulting state is better than expected, we want to increase it proportionally. If it is less than expected, we want to decrease it proportionally. Thus, we multiply the gradient of the parameter weights w.r.t. the taken action's probability by the difference of the return for that state-action pair.\n",
    "\n",
    "Importantly, there is an extra factor however that we must consider; when we decide that we want to take the gradient of the parameters w.r.t. a specific action's return, the policy expectancy must be multiplied by the specific action's likelihood to determine the value it contributes to the policy. Thus, we end up with the gradient of the action's probability conditioned on the state and parameters. \n",
    "\n",
    "Thus, the general concept of loss to backpropogate in the REINFORCE algorithm is:\n",
    "\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\frac{\\nabla\\pi(A_t|S_t, \\theta)}{\\pi(A_t|S_t, \\theta)}$\n",
    "\n",
    "This can be expressed as:\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}) \\nabla \\ln{\\pi(A_t|S_t, \\theta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below just worries about the loss and not the gradient, as PyTorch provides autograd differntiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_loss(prob, state_util_difference):\n",
    "    return nn.NLLLoss(prob, torch.ones(1)) * state_util_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the network for approximating state utililities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateUtility(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StateUtility, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.state_utility = nn.Sequential(\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4), # TODO: try reducing to one hidden layer if learning proves initially dificult\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1), # output a tensor of a scalar value\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        state_utility = self.state_utility(x)\n",
    "        return state_utility\n",
    "\n",
    "state_util_model = StateUtility().to(device)\n",
    "state_util_adam = torch.optim.Adam(state_util_model.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the state utilities network, we just use L1 loss with the gradients of W with respect to state utility.\n",
    "\n",
    "$\\Large (G_t - \\hat{\\upsilon}(S_t, W)) \\nabla \\hat{\\upsilon}(S_t, W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like above, the code below just worries about the loss and not the gradient, as PyTorch provides autograd differntiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_util_loss(calculated_state_value, episode_state_value):\n",
    "    # the overall state value is the input, and the individual state value is our target\n",
    "    return nn.L1Loss(calculated_state_value, episode_state_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to calculate a state instance's utility in a given episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ep_state_util():\n",
    "    \"\"\"Given an observation and reward from an\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the Lunar Lander environment now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jhotz/School/cpsc_532j/hm2_assignment/env/lib/python3.10/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/Users/jhotz/School/cpsc_532j/hm2_assignment/env/lib/python3.10/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m timestep \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(ep_length)):\n\u001b[1;32m     27\u001b[0m     terminal \u001b[39m=\u001b[39m timestep \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(rewards) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> 28\u001b[0m     state_utilities[timestep] \u001b[39m=\u001b[39m rewards[timestep] \u001b[39m+\u001b[39m (gamma \u001b[39m*\u001b[39m state_utilities[timestep\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal \u001b[39melse\u001b[39;00m rewards[timestep]\n\u001b[1;32m     29\u001b[0m     pred_state_utility \u001b[39m=\u001b[39m state_util_model(torch\u001b[39m.\u001b[39mtensor(observations[timestep]))\n\u001b[1;32m     30\u001b[0m     loss_state_utility \u001b[39m=\u001b[39m state_util_loss(pred_state_utility, state_utilities[timestep])\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "# TODO: use a custom dataloader class and see if speed up\n",
    "\n",
    "env = gym.make(\n",
    "    \"LunarLander-v2\"\n",
    ")\n",
    "\n",
    "action_space_seed = np.random.seed(13)\n",
    "\n",
    "observation, info = env.reset(seed=13)\n",
    "\n",
    "# index i in the lists below corresponds to the timestep i of the current episode\n",
    "observations = []\n",
    "rewards = []\n",
    "state_utilities = []\n",
    "\n",
    "for timestep in range(1000):\n",
    "    action_weights = np.array(policy_model(torch.tensor(observation)).tolist())\n",
    "    action_array = np.random.multinomial(n=1, pvals=action_weights)\n",
    "    action = np.argmax(action_array)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    observations.append(observation)\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        ep_length = len(observations)\n",
    "        for timestep in reversed(range(ep_length)):\n",
    "            terminal = timestep == len(rewards) - 1\n",
    "            state_utilities[timestep] = rewards[timestep] + (gamma * state_utilities[timestep+1]) if not terminal else rewards[timestep]\n",
    "            pred_state_utility = state_util_model(torch.tensor(observations[timestep]))\n",
    "            loss_state_utility = state_util_loss(pred_state_utility, state_utilities[timestep])\n",
    "            state_util_adam.zero_grad()\n",
    "            loss_state_utility.backward()\n",
    "            state_util_adam.step()\n",
    "        observation, info = env.reset()\n",
    "        observations, rewards, utilities = []\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00c7148fe7d049885671e82bbf6f02dbbdff16ff92bf68e1f2741c72b6e7373b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
